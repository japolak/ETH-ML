For this regression project we tried several different approaches. Firstly linear regression wasn't really successful due to a large number of features. Therefore, we secondly tried to implement variable selection regression models using Lasso, Ridge and Elastic Nets, which outperformed the medium benchmark. Thirdly, the implementation of Random Forest showed promising results. We used the variable importance of Random Forest with a threshold of 100 for variable selection. Lastly, Gradient Boosting Trees with fine-tuned parameters was selected for the final model. The parameters for our
Boosted Trees were following: Learning rate 0.01, max depth of a tree 4, number of trees 5000. Furthermore, some data pre-processing was performed. Missing values were replaced by columns median. Outlier values were detected using box-plots and removed from the training dataset. Attempt to transform dataset using powers, logs and so was made, but such data did not perform significantly better or worse. 5-Fold cross-validation was used in several places to estimate test error, fit boosted trees and to fine-tune parameters.
