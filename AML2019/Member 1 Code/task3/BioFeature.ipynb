{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils.validation import check_array\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from biosppy.signals import ecg  # preprocessing ECG signal\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioFeature():\n",
    "    def __init__(self, sample_rate=300,\n",
    "                 threshold=0.48, segstart=90, segend=110):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.threshold = threshold\n",
    "        self.segstart = segstart  # Q-R interval\n",
    "        self.segend = segend      # R-S interval\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = check_array(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        # X = check_array(X)\n",
    "        T = 1.0 / self.sample_rate\n",
    "        bio_feature = np.empty(1)\n",
    "        for i in range(X.shape[0]):\n",
    "            if i % 50 == 0:\n",
    "                print(\"processed %d\" % i)\n",
    "            Xi = X[i]\n",
    "            rpeaks = ecg.engzee_segmenter(Xi,\n",
    "                                          sampling_rate=300,\n",
    "                                          threshold=self.threshold)\n",
    "            rpeaks = rpeaks[0]\n",
    "            # 1. compute the mean of DFT for one QRS wave\n",
    "            QR_int = self.segstart    # Q-R interval\n",
    "            RS_int = self.segend      # R-S interval\n",
    "            m_idx = X.shape[1] - RS_int\n",
    "            idx = np.where((rpeaks > QR_int) & (rpeaks < m_idx))\n",
    "            idx = idx[0]\n",
    "            new_rpeaks = rpeaks[idx]\n",
    "            _fft = np.zeros(QR_int + RS_int)\n",
    "            for j in range(len(new_rpeaks)):\n",
    "                # extract segment around rpeaks\n",
    "                QRS = Xi[new_rpeaks[j] - QR_int:new_rpeaks[j] + RS_int]\n",
    "                # compute the mean of DFT\n",
    "                fft = np.fft.fft(QRS)\n",
    "                fft_amp = np.absolute(fft)\n",
    "                if j == 0:\n",
    "                    _fft = fft_amp\n",
    "                else:\n",
    "                    _fft = np.vstack((_fft, fft_amp))\n",
    "            fft_mean = np.mean(_fft, axis=0)\n",
    "            if len(new_rpeaks) == 0 or len(new_rpeaks) == 1:\n",
    "                fft_mean = _fft\n",
    "                \n",
    "            # 2. compute the statistics of heart beat\n",
    "            out = ecg.ecg(signal=Xi, sampling_rate=300,\n",
    "                          show=False)\n",
    "            [ts, sig, rpeaks, temp_ts, temp, hr_ts, heart_rate] = out\n",
    "            heart_stats = [0, 0, 0, 0]\n",
    "            if len(heart_rate) != 0:\n",
    "                heart_stats[0] = np.mean(heart_rate)\n",
    "                heart_stats[1] = np.var(heart_rate)\n",
    "                heart_stats[2] = np.amin(heart_rate)\n",
    "                heart_stats[3] = np.amax(heart_rate)\n",
    "            heart_stats = np.asarray(heart_stats)\n",
    "\n",
    "            # 3. compute the statistics of R-R interval\n",
    "            RR_int = []\n",
    "            for k in range(1, len(rpeaks)):\n",
    "                RR_int.append(T * (rpeaks[k] - rpeaks[k-1]))\n",
    "            RR_int = np.asarray(RR_int)\n",
    "            RR_stats = [0, 0, 0, 0]\n",
    "            if len(RR_int) != 0:\n",
    "                RR_stats[0] = np.mean(RR_int)\n",
    "                RR_stats[1] = np.var(RR_int)\n",
    "                RR_stats[2] = np.amin(RR_int)\n",
    "                RR_stats[3] = np.amax(RR_int)\n",
    "            RR_stats = np.asarray(RR_stats)\n",
    "\n",
    "            feature = np.hstack((fft_mean, heart_stats))\n",
    "            feature = np.hstack((feature, RR_stats))\n",
    "            if i == 0:\n",
    "                bio_feature = feature\n",
    "            else:\n",
    "                bio_feature = np.vstack((bio_feature, feature))\n",
    "        return bio_feature\n",
    "fs = BioFeature()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HRVFeature(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Adpated from <<van Gent, P. (2016).\n",
    "    Analyzing a Discrete Heart Rate Signal Using Python.\n",
    "    A tech blog about fun things with Python and embedded electronics>>\"\"\"\n",
    "    \"\"\" feature includes: BPM, IBI, SDNN. SDSD, RMSSD, pNN50, pNN20 \"\"\"\n",
    "    def __init__(self, sample_rate=300,\n",
    "                 start=40, end=50, save_path=None):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.segstart = start\n",
    "        self.segend = end\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = check_array(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        T = 1.0 / self.sample_rate\n",
    "        bio_feature = np.empty(1)\n",
    "        for i in range(X.shape[0]):\n",
    "            if i % 50 == 0:\n",
    "                print(\"processed %d\" % i)\n",
    "            Xi = X[i, :]\n",
    "            # extract the R-peaks positions\n",
    "            out = ecg.ecg(signal=Xi, sampling_rate=300,\n",
    "                          show=False)\n",
    "            [ts, sig, rpeaks, temp_ts, temp, hr_ts, heart_rate] = out\n",
    "            # 1. Extract Time Domain Measures\n",
    "            # Compute the R-R interval (ms)\n",
    "            RR_int = []\n",
    "            for k in range(1, len(rpeaks)):\n",
    "                RR_int.append(1000.0 * T * (rpeaks[k] - rpeaks[k - 1]))\n",
    "            RR_int = np.asarray(RR_int)\n",
    "            # Compute the R-R interval difference\n",
    "            RR_diff = []    # difference between adajacent RR interval\n",
    "            RR_sqdiff = []  # squared difference of RR interval\n",
    "            for k in range(1, len(RR_int)):\n",
    "                m_diff = RR_int[k] - RR_int[k - 1]\n",
    "                RR_diff.append(abs(m_diff))\n",
    "                RR_sqdiff.append(math.pow(m_diff, 2))\n",
    "            RR_diff = np.asarray(RR_diff)\n",
    "            RR_sqdiff = np.asarray(RR_sqdiff)\n",
    "\n",
    "            bio_feature = np.zeros(10)\n",
    "            if len(RR_int) >= 1:\n",
    "                # 1.ibi measure: the mean of the R-R interval\n",
    "                ibi = np.mean(RR_int)\n",
    "                bio_feature[0] = ibi\n",
    "\n",
    "                # 2.sdn measure: the standard deviation of R-R intervals\n",
    "                sdnn = np.std(RR_int)\n",
    "                bio_feature[1] = sdnn\n",
    "\n",
    "                # 3. max of the R-R interval\n",
    "                bio_feature[2] = np.amax(RR_int)\n",
    "\n",
    "                # data = pd.Series(RR_int)\n",
    "                # arma_mod4 = sm.tsa.ARMA(data, (4,0)).fit()\n",
    "                # bio_feature[3] = arma_mod4.params[0]\n",
    "                # bio_feature[4] = arma_mod4.params[1]\n",
    "                # bio_feature[5] = arma_mod4.params[2]\n",
    "                # bio_feature[6] = arma_mod4.params[3]\n",
    "\n",
    "                if len(RR_diff) >= 1:\n",
    "                    # 5.sdsd measure: the standard deviation of the R-R diff\n",
    "                    sdsd = np.std(RR_diff)\n",
    "                    bio_feature[3] = sdsd\n",
    "\n",
    "                    # 6. the mean , min, max of the R-R difference\n",
    "                    bio_feature[4] = np.mean(RR_diff)\n",
    "                    bio_feature[5] = np.amax(RR_diff)\n",
    "\n",
    "                    # 4.rsmd measure: the root mean square of R-R differences\n",
    "                    rmssd = np.sqrt(np.mean(RR_sqdiff))\n",
    "                    bio_feature[6] = rmssd\n",
    "\n",
    "                    # 5. pnn20/50: the percentage of differences > 50/20 ms\n",
    "                    nn20 = [x for x in RR_diff if (x > 20)]\n",
    "                    nn50 = [x for x in RR_diff if (x > 50)]\n",
    "                    pnn20 = float(len(nn20)) / float(len(RR_diff))\n",
    "                    pnn50 = float(len(nn50)) / float(len(RR_diff))\n",
    "                    bio_feature[7] = pnn20\n",
    "                    bio_feature[8] = pnn50\n",
    "\n",
    "                # 6. bpm measure: the average heart rate per minute\n",
    "                # bpm = np.mean(heart_rate)\n",
    "                bpm = 60000 / np.mean(RR_int)\n",
    "                bio_feature[9] = bpm\n",
    "\n",
    "                # 7. extract the QRS waves and use the mean as feature\n",
    "                QR_int = self.segstart    # Q-R interval\n",
    "                RS_int = self.segend      # R-S interval\n",
    "                m_idx = X.shape[1] - RS_int\n",
    "                idx = np.where((rpeaks > QR_int) & (rpeaks < m_idx))\n",
    "                idx = idx[0]\n",
    "                new_rpeaks = rpeaks[idx]\n",
    "                nQRS = np.zeros(QR_int + RS_int)\n",
    "                for j in range(len(new_rpeaks)):\n",
    "                    # extract segment around rpeaks\n",
    "                    QRS = Xi[new_rpeaks[j] - QR_int:new_rpeaks[j] + RS_int]\n",
    "                    # QRS = np.absolute(np.fft.fft(QRS))\n",
    "                    # compute the mean of DFT\n",
    "                    if j == 0:\n",
    "                        nQRS = QRS\n",
    "                    else:\n",
    "                        nQRS = np.vstack((nQRS, QRS))\n",
    "                QRS_mean = np.mean(nQRS, axis=0)\n",
    "                if len(new_rpeaks) == 0 or len(new_rpeaks) == 1:\n",
    "                    QRS_mean = nQRS\n",
    "                bio_feature = np.hstack((bio_feature, QRS_mean))\n",
    "\n",
    "            for j in range(len(bio_feature)):\n",
    "                if np.isnan(bio_feature[j]):\n",
    "                    print(\"Error in extracting sample\")\n",
    "                    print(i)\n",
    "                    print(\":found NaN in bio_feature\")\n",
    "            if i == 0:\n",
    "                features = bio_feature\n",
    "            else:\n",
    "                features = np.vstack((features, bio_feature))\n",
    "        return features\n",
    "hrvfs = HRVFeature()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "signals = []\n",
    "with open(\"data/X_train.csv\") as f_train:\n",
    "    for line in f_train.readlines()[1:]:\n",
    "        s = list(map(int, line.split(',')[1:]))\n",
    "        if len(s) < 18155:\n",
    "            s.extend([0 for x in range(len(s), 18155)])\n",
    "        signals.append(s)\n",
    "signals = np.array([np.array(s) for s in signals])\n",
    "y = np.loadtxt('data/y_train.csv', delimiter=',', skiprows=1, usecols=range(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 0\n",
      "processed 50\n",
      "processed 100\n",
      "processed 150\n",
      "processed 200\n",
      "processed 250\n",
      "processed 300\n",
      "processed 350\n",
      "processed 400\n",
      "processed 450\n",
      "processed 500\n",
      "processed 550\n",
      "processed 600\n",
      "processed 650\n",
      "processed 700\n",
      "processed 750\n",
      "processed 800\n",
      "processed 850\n",
      "processed 900\n",
      "processed 950\n",
      "processed 1000\n",
      "processed 1050\n",
      "processed 1100\n",
      "processed 1150\n",
      "processed 1200\n",
      "processed 1250\n",
      "processed 1300\n",
      "processed 1350\n",
      "processed 1400\n",
      "processed 1450\n",
      "processed 1500\n",
      "processed 1550\n",
      "processed 1600\n",
      "processed 1650\n",
      "processed 1700\n",
      "processed 1750\n",
      "processed 1800\n",
      "processed 1850\n",
      "processed 1900\n",
      "processed 1950\n",
      "processed 2000\n",
      "processed 2050\n",
      "processed 2100\n",
      "processed 2150\n",
      "processed 2200\n",
      "processed 2250\n",
      "processed 2300\n",
      "processed 2350\n",
      "processed 2400\n",
      "processed 2450\n",
      "processed 2500\n",
      "processed 2550\n",
      "processed 2600\n",
      "processed 2650\n",
      "processed 2700\n",
      "processed 2750\n",
      "processed 2800\n",
      "processed 2850\n",
      "processed 2900\n",
      "processed 2950\n",
      "processed 3000\n",
      "processed 3050\n",
      "processed 3100\n",
      "processed 3150\n",
      "processed 3200\n",
      "processed 3250\n",
      "processed 3300\n",
      "processed 3350\n",
      "processed 3400\n",
      "processed 3450\n",
      "processed 3500\n",
      "processed 3550\n",
      "processed 3600\n",
      "processed 3650\n",
      "processed 3700\n",
      "processed 3750\n",
      "processed 3800\n",
      "processed 3850\n",
      "processed 3900\n",
      "processed 3950\n",
      "processed 4000\n",
      "processed 4050\n",
      "processed 4100\n",
      "processed 4150\n",
      "processed 4200\n",
      "processed 4250\n",
      "processed 4300\n",
      "processed 4350\n",
      "processed 4400\n",
      "processed 4450\n",
      "processed 4500\n",
      "processed 4550\n",
      "processed 4600\n",
      "processed 4650\n",
      "processed 4700\n",
      "processed 4750\n",
      "processed 4800\n",
      "processed 4850\n",
      "processed 4900\n",
      "processed 4950\n",
      "processed 5000\n",
      "processed 5050\n",
      "processed 5100\n"
     ]
    }
   ],
   "source": [
    "bioFeatures = fs.transform(signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 0\n",
      "processed 50\n",
      "processed 100\n",
      "processed 150\n",
      "processed 200\n",
      "processed 250\n",
      "processed 300\n",
      "processed 350\n",
      "processed 400\n",
      "processed 450\n",
      "processed 500\n",
      "processed 550\n",
      "processed 600\n",
      "processed 650\n",
      "processed 700\n",
      "processed 750\n",
      "processed 800\n",
      "processed 850\n",
      "processed 900\n",
      "processed 950\n",
      "processed 1000\n",
      "processed 1050\n",
      "processed 1100\n",
      "processed 1150\n",
      "processed 1200\n",
      "processed 1250\n",
      "processed 1300\n",
      "processed 1350\n",
      "processed 1400\n",
      "processed 1450\n",
      "processed 1500\n",
      "processed 1550\n",
      "processed 1600\n",
      "processed 1650\n",
      "processed 1700\n",
      "processed 1750\n",
      "processed 1800\n",
      "processed 1850\n",
      "processed 1900\n",
      "processed 1950\n",
      "processed 2000\n",
      "processed 2050\n",
      "processed 2100\n",
      "processed 2150\n",
      "processed 2200\n",
      "processed 2250\n",
      "processed 2300\n",
      "processed 2350\n",
      "processed 2400\n",
      "processed 2450\n",
      "processed 2500\n",
      "processed 2550\n",
      "processed 2600\n",
      "processed 2650\n",
      "processed 2700\n",
      "processed 2750\n",
      "processed 2800\n",
      "processed 2850\n",
      "processed 2900\n",
      "processed 2950\n",
      "processed 3000\n",
      "processed 3050\n",
      "processed 3100\n",
      "processed 3150\n",
      "processed 3200\n",
      "processed 3250\n",
      "processed 3300\n",
      "processed 3350\n",
      "processed 3400\n",
      "processed 3450\n",
      "processed 3500\n",
      "processed 3550\n",
      "processed 3600\n",
      "processed 3650\n",
      "processed 3700\n",
      "processed 3750\n",
      "processed 3800\n",
      "processed 3850\n",
      "processed 3900\n",
      "processed 3950\n",
      "processed 4000\n",
      "processed 4050\n",
      "processed 4100\n",
      "processed 4150\n",
      "processed 4200\n",
      "processed 4250\n",
      "processed 4300\n",
      "processed 4350\n",
      "processed 4400\n",
      "processed 4450\n",
      "processed 4500\n",
      "processed 4550\n",
      "processed 4600\n",
      "processed 4650\n",
      "processed 4700\n",
      "processed 4750\n",
      "processed 4800\n",
      "processed 4850\n",
      "processed 4900\n",
      "processed 4950\n",
      "processed 5000\n",
      "processed 5050\n",
      "processed 5100\n"
     ]
    }
   ],
   "source": [
    "hrvFeatures = hrvfs.transform(signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"bioFeatures.csv\", \"w\") as f:\n",
    "    for idx,vals in enumerate(bioFeatures):\n",
    "        s = str(idx)\n",
    "        for v in vals:\n",
    "            s += \",%f\" % v\n",
    "        s += \"\\n\"\n",
    "        f.write(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"hrvFeatures.csv\", \"w\") as f:\n",
    "    for idx,vals in enumerate(hrvFeatures):\n",
    "        s = str(idx)\n",
    "        for v in vals:\n",
    "            s += \",%f\" % v\n",
    "        s += \"\\n\"\n",
    "        f.write(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.loadtxt('data/y_train.csv', delimiter=',', skiprows=1, usecols=range(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_add = np.loadtxt('data/rpeakfeature.csv', delimiter=',', skiprows=1, usecols=range(1,9))\n",
    "newX = []\n",
    "for i in range(len(X)):\n",
    "     newX.append(np.concatenate([X[i], X_add[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 0\n",
      "processed 50\n",
      "processed 100\n",
      "processed 150\n",
      "processed 200\n",
      "processed 250\n",
      "processed 300\n",
      "processed 350\n",
      "processed 400\n",
      "processed 450\n",
      "processed 500\n",
      "processed 550\n",
      "processed 600\n",
      "processed 650\n",
      "processed 700\n",
      "processed 750\n",
      "processed 800\n",
      "processed 850\n",
      "processed 900\n",
      "processed 950\n",
      "processed 1000\n",
      "processed 1050\n",
      "processed 1100\n",
      "processed 1150\n",
      "processed 1200\n",
      "processed 1250\n",
      "processed 1300\n",
      "processed 1350\n",
      "processed 1400\n",
      "processed 1450\n",
      "processed 1500\n",
      "processed 1550\n",
      "processed 1600\n",
      "processed 1650\n",
      "processed 1700\n",
      "processed 1750\n",
      "processed 1800\n",
      "processed 1850\n",
      "processed 1900\n",
      "processed 1950\n",
      "processed 2000\n",
      "processed 2050\n",
      "processed 2100\n",
      "processed 2150\n",
      "processed 2200\n",
      "processed 2250\n",
      "processed 2300\n",
      "processed 2350\n",
      "processed 2400\n",
      "processed 2450\n",
      "processed 2500\n",
      "processed 2550\n",
      "processed 2600\n",
      "processed 2650\n",
      "processed 2700\n",
      "processed 2750\n",
      "processed 2800\n",
      "processed 2850\n",
      "processed 2900\n",
      "processed 2950\n",
      "processed 3000\n",
      "processed 3050\n",
      "processed 3100\n",
      "processed 3150\n",
      "processed 3200\n",
      "processed 3250\n",
      "processed 3300\n",
      "processed 3350\n",
      "processed 3400\n",
      "processed 3450\n",
      "processed 3500\n",
      "processed 3550\n",
      "processed 3600\n",
      "processed 3650\n",
      "processed 3700\n",
      "processed 3750\n",
      "processed 3800\n",
      "processed 3850\n",
      "processed 3900\n",
      "processed 3950\n",
      "processed 4000\n",
      "processed 4050\n",
      "processed 4100\n",
      "processed 4150\n",
      "processed 4200\n",
      "processed 4250\n",
      "processed 4300\n",
      "processed 4350\n",
      "processed 4400\n",
      "processed 4450\n",
      "processed 4500\n",
      "processed 4550\n",
      "processed 4600\n",
      "processed 4650\n",
      "processed 4700\n",
      "processed 4750\n",
      "processed 4800\n",
      "processed 4850\n",
      "processed 4900\n",
      "processed 4950\n",
      "processed 5000\n",
      "processed 5050\n",
      "processed 5100\n"
     ]
    }
   ],
   "source": [
    "X_add2 = hrvfs.transform(np.array([np.array(s) for s in signals]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "newX2 = []\n",
    "for i in range(len(X)):\n",
    "     newX2.append(np.concatenate([newX[i], X_add2[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-merror:0.203326\teval-merror:0.23505\ttrain-f1:0.796674\teval-f1:0.76495\n",
      "[1]\ttrain-merror:0.182614\teval-merror:0.213144\ttrain-f1:0.817386\teval-f1:0.786856\n",
      "[2]\ttrain-merror:0.17853\teval-merror:0.210184\ttrain-f1:0.82147\teval-f1:0.789816\n",
      "[3]\ttrain-merror:0.173279\teval-merror:0.208999\ttrain-f1:0.826721\teval-f1:0.791001\n",
      "[4]\ttrain-merror:0.164819\teval-merror:0.206631\ttrain-f1:0.835181\teval-f1:0.793369\n",
      "[5]\ttrain-merror:0.162777\teval-merror:0.201303\ttrain-f1:0.837223\teval-f1:0.798697\n",
      "[6]\ttrain-merror:0.155193\teval-merror:0.20071\ttrain-f1:0.844807\teval-f1:0.79929\n",
      "[7]\ttrain-merror:0.153151\teval-merror:0.199526\ttrain-f1:0.846849\teval-f1:0.800474\n",
      "[8]\ttrain-merror:0.1479\teval-merror:0.19775\ttrain-f1:0.8521\teval-f1:0.80225\n",
      "[9]\ttrain-merror:0.144399\teval-merror:0.188869\ttrain-f1:0.855601\teval-f1:0.811131\n",
      "[10]\ttrain-merror:0.140023\teval-merror:0.190053\ttrain-f1:0.859977\teval-f1:0.809947\n",
      "[11]\ttrain-merror:0.139148\teval-merror:0.192422\ttrain-f1:0.860852\teval-f1:0.807578\n",
      "[12]\ttrain-merror:0.129813\teval-merror:0.187093\ttrain-f1:0.870187\teval-f1:0.812907\n",
      "[13]\ttrain-merror:0.12748\teval-merror:0.189461\ttrain-f1:0.87252\teval-f1:0.810539\n",
      "[14]\ttrain-merror:0.12252\teval-merror:0.187685\ttrain-f1:0.87748\teval-f1:0.812315\n",
      "[15]\ttrain-merror:0.11902\teval-merror:0.182948\ttrain-f1:0.88098\teval-f1:0.817052\n",
      "[16]\ttrain-merror:0.116686\teval-merror:0.185909\ttrain-f1:0.883314\teval-f1:0.814091\n",
      "[17]\ttrain-merror:0.111727\teval-merror:0.184133\ttrain-f1:0.888273\teval-f1:0.815867\n",
      "[18]\ttrain-merror:0.106184\teval-merror:0.183541\ttrain-f1:0.893816\teval-f1:0.816459\n",
      "[19]\ttrain-merror:0.105309\teval-merror:0.183541\ttrain-f1:0.894691\teval-f1:0.816459\n",
      "[20]\ttrain-merror:0.1021\teval-merror:0.181172\ttrain-f1:0.8979\teval-f1:0.818828\n",
      "[21]\ttrain-merror:0.102392\teval-merror:0.184133\ttrain-f1:0.897608\teval-f1:0.815867\n",
      "[22]\ttrain-merror:0.099183\teval-merror:0.186501\ttrain-f1:0.900817\teval-f1:0.813499\n",
      "[23]\ttrain-merror:0.099475\teval-merror:0.182356\ttrain-f1:0.900525\teval-f1:0.817644\n",
      "[24]\ttrain-merror:0.095099\teval-merror:0.181764\ttrain-f1:0.904901\teval-f1:0.818236\n",
      "[25]\ttrain-merror:0.092182\teval-merror:0.183541\ttrain-f1:0.907818\teval-f1:0.816459\n",
      "[26]\ttrain-merror:0.09014\teval-merror:0.182356\ttrain-f1:0.90986\teval-f1:0.817644\n",
      "[27]\ttrain-merror:0.088681\teval-merror:0.178804\ttrain-f1:0.911319\teval-f1:0.821196\n",
      "[28]\ttrain-merror:0.088098\teval-merror:0.17762\ttrain-f1:0.911902\teval-f1:0.82238\n",
      "[29]\ttrain-merror:0.083722\teval-merror:0.17762\ttrain-f1:0.916278\teval-f1:0.82238\n",
      "[30]\ttrain-merror:0.082264\teval-merror:0.179988\ttrain-f1:0.917736\teval-f1:0.820012\n",
      "[31]\ttrain-merror:0.082264\teval-merror:0.182356\ttrain-f1:0.917736\teval-f1:0.817644\n",
      "[32]\ttrain-merror:0.077888\teval-merror:0.181172\ttrain-f1:0.922112\teval-f1:0.818828\n",
      "[33]\ttrain-merror:0.077305\teval-merror:0.179988\ttrain-f1:0.922695\teval-f1:0.820012\n",
      "[34]\ttrain-merror:0.075846\teval-merror:0.18058\ttrain-f1:0.924154\teval-f1:0.81942\n",
      "[35]\ttrain-merror:0.074387\teval-merror:0.18058\ttrain-f1:0.925613\teval-f1:0.81942\n",
      "[36]\ttrain-merror:0.072637\teval-merror:0.18058\ttrain-f1:0.927363\teval-f1:0.81942\n",
      "[37]\ttrain-merror:0.072345\teval-merror:0.17762\ttrain-f1:0.927655\teval-f1:0.82238\n",
      "[38]\ttrain-merror:0.068845\teval-merror:0.177028\ttrain-f1:0.931155\teval-f1:0.822972\n",
      "[39]\ttrain-merror:0.065928\teval-merror:0.18058\ttrain-f1:0.934072\teval-f1:0.81942\n",
      "[40]\ttrain-merror:0.063886\teval-merror:0.178212\ttrain-f1:0.936114\teval-f1:0.821788\n",
      "[41]\ttrain-merror:0.063302\teval-merror:0.179396\ttrain-f1:0.936698\teval-f1:0.820604\n",
      "[42]\ttrain-merror:0.060968\teval-merror:0.179988\ttrain-f1:0.939032\teval-f1:0.820012\n",
      "[43]\ttrain-merror:0.059802\teval-merror:0.17762\ttrain-f1:0.940198\teval-f1:0.82238\n",
      "[44]\ttrain-merror:0.058051\teval-merror:0.179396\ttrain-f1:0.941949\teval-f1:0.820604\n",
      "[45]\ttrain-merror:0.056009\teval-merror:0.181764\ttrain-f1:0.943991\teval-f1:0.818236\n",
      "[46]\ttrain-merror:0.055718\teval-merror:0.18058\ttrain-f1:0.944282\teval-f1:0.81942\n",
      "[47]\ttrain-merror:0.053967\teval-merror:0.182948\ttrain-f1:0.946033\teval-f1:0.817052\n",
      "[48]\ttrain-merror:0.053967\teval-merror:0.183541\ttrain-f1:0.946033\teval-f1:0.816459\n",
      "[49]\ttrain-merror:0.052509\teval-merror:0.181764\ttrain-f1:0.947491\teval-f1:0.818236\n",
      "[50]\ttrain-merror:0.052509\teval-merror:0.179988\ttrain-f1:0.947491\teval-f1:0.820012\n",
      "[51]\ttrain-merror:0.051634\teval-merror:0.18058\ttrain-f1:0.948366\teval-f1:0.81942\n",
      "[52]\ttrain-merror:0.049592\teval-merror:0.17762\ttrain-f1:0.950408\teval-f1:0.82238\n",
      "[53]\ttrain-merror:0.047841\teval-merror:0.181172\ttrain-f1:0.952159\teval-f1:0.818828\n",
      "[54]\ttrain-merror:0.046966\teval-merror:0.18058\ttrain-f1:0.953034\teval-f1:0.81942\n",
      "[55]\ttrain-merror:0.045216\teval-merror:0.18058\ttrain-f1:0.954784\teval-f1:0.81942\n",
      "[56]\ttrain-merror:0.044924\teval-merror:0.179988\ttrain-f1:0.955076\teval-f1:0.820012\n",
      "[57]\ttrain-merror:0.044049\teval-merror:0.181172\ttrain-f1:0.955951\teval-f1:0.818828\n",
      "[58]\ttrain-merror:0.041424\teval-merror:0.178804\ttrain-f1:0.958576\teval-f1:0.821196\n",
      "[59]\ttrain-merror:0.041132\teval-merror:0.181764\ttrain-f1:0.958868\teval-f1:0.818236\n",
      "[60]\ttrain-merror:0.040257\teval-merror:0.18058\ttrain-f1:0.959743\teval-f1:0.81942\n",
      "[61]\ttrain-merror:0.040257\teval-merror:0.182356\ttrain-f1:0.959743\teval-f1:0.817644\n",
      "[62]\ttrain-merror:0.040257\teval-merror:0.181172\ttrain-f1:0.959743\teval-f1:0.818828\n",
      "[63]\ttrain-merror:0.03909\teval-merror:0.179396\ttrain-f1:0.96091\teval-f1:0.820604\n",
      "[64]\ttrain-merror:0.03909\teval-merror:0.177028\ttrain-f1:0.96091\teval-f1:0.822972\n",
      "[65]\ttrain-merror:0.038798\teval-merror:0.179396\ttrain-f1:0.961202\teval-f1:0.820604\n",
      "[66]\ttrain-merror:0.03734\teval-merror:0.175252\ttrain-f1:0.96266\teval-f1:0.824748\n",
      "[67]\ttrain-merror:0.035589\teval-merror:0.177028\ttrain-f1:0.964411\teval-f1:0.822972\n",
      "[68]\ttrain-merror:0.035589\teval-merror:0.177028\ttrain-f1:0.964411\teval-f1:0.822972\n",
      "[69]\ttrain-merror:0.035006\teval-merror:0.175252\ttrain-f1:0.964994\teval-f1:0.824748\n",
      "[70]\ttrain-merror:0.035006\teval-merror:0.179988\ttrain-f1:0.964994\teval-f1:0.820012\n",
      "[71]\ttrain-merror:0.033839\teval-merror:0.179396\ttrain-f1:0.966161\teval-f1:0.820604\n",
      "[72]\ttrain-merror:0.033256\teval-merror:0.178804\ttrain-f1:0.966744\teval-f1:0.821196\n",
      "[73]\ttrain-merror:0.030922\teval-merror:0.178804\ttrain-f1:0.969078\teval-f1:0.821196\n",
      "[74]\ttrain-merror:0.029172\teval-merror:0.179396\ttrain-f1:0.970828\teval-f1:0.820604\n",
      "[75]\ttrain-merror:0.02713\teval-merror:0.17762\ttrain-f1:0.97287\teval-f1:0.82238\n",
      "[76]\ttrain-merror:0.026254\teval-merror:0.17762\ttrain-f1:0.973746\teval-f1:0.82238\n",
      "[77]\ttrain-merror:0.026546\teval-merror:0.178212\ttrain-f1:0.973454\teval-f1:0.821788\n",
      "[78]\ttrain-merror:0.026254\teval-merror:0.176436\ttrain-f1:0.973746\teval-f1:0.823564\n",
      "[79]\ttrain-merror:0.025963\teval-merror:0.178804\ttrain-f1:0.974037\teval-f1:0.821196\n",
      "Test error using softmax = 0.8211959739490823\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_X, eval_X, train_Y, eval_Y = train_test_split(newX2, y, test_size=0.33, random_state=42)\n",
    "xg_train = xgb.DMatrix(train_X, label=train_Y)\n",
    "xg_eval = xgb.DMatrix(eval_X, label=eval_Y)\n",
    "\n",
    "# setup parameters for xgboost\n",
    "param = {}\n",
    "# use softmax multi-class classification\n",
    "param['objective'] = 'multi:softmax'\n",
    "# scale weight of positive examples\n",
    "param['eta'] = 0.2\n",
    "param['gamma'] = 1.0\n",
    "param['max_depth'] = 6\n",
    "param['silent'] = 1\n",
    "param['subsample'] = 0.8\n",
    "param['colsample_bytree'] = 0.9\n",
    "param['min_child_weight'] = 20\n",
    "param['num_class'] = 4\n",
    "\n",
    "\n",
    "watchlist = [(xg_train, 'train'), (xg_eval, 'eval')]\n",
    "num_round = 80\n",
    "bst = xgb.train(param, \n",
    "                xg_train, \n",
    "                num_round, \n",
    "                watchlist, \n",
    "                feval=lambda y,t: (\"f1\", f1_score(y, t.get_label(), average='micro')))\n",
    "\n",
    "# get prediction\n",
    "pred = bst.predict(xg_eval)\n",
    "# error_rate = np.sum(pred != test_Y) / test_Y.shape[0]\n",
    "F1 = f1_score(eval_Y, pred, average='micro')\n",
    "print('Test error using softmax = {}'.format(F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "testsignals = []\n",
    "with open(\"data/X_test.csv\") as f_test:\n",
    "    for line in f_test.readlines()[1:]:\n",
    "        s = list(map(int, line.split(',')[1:]))\n",
    "        if len(s) < 18155:\n",
    "            s.extend([0 for x in range(len(s), 18155)])\n",
    "        testsignals.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 0\n",
      "processed 50\n",
      "processed 100\n",
      "processed 150\n",
      "processed 200\n",
      "processed 250\n",
      "processed 300\n",
      "processed 350\n",
      "processed 400\n",
      "processed 450\n",
      "processed 500\n",
      "processed 550\n",
      "processed 600\n",
      "processed 650\n",
      "processed 700\n",
      "processed 750\n",
      "processed 800\n",
      "processed 850\n",
      "processed 900\n",
      "processed 950\n",
      "processed 1000\n",
      "processed 1050\n",
      "processed 1100\n",
      "processed 1150\n",
      "processed 1200\n",
      "processed 1250\n",
      "processed 1300\n",
      "processed 1350\n",
      "processed 1400\n",
      "processed 1450\n",
      "processed 1500\n",
      "processed 1550\n",
      "processed 1600\n",
      "processed 1650\n",
      "processed 1700\n",
      "processed 1750\n",
      "processed 1800\n",
      "processed 1850\n",
      "processed 1900\n",
      "processed 1950\n",
      "processed 2000\n",
      "processed 2050\n",
      "processed 2100\n",
      "processed 2150\n",
      "processed 2200\n",
      "processed 2250\n",
      "processed 2300\n",
      "processed 2350\n",
      "processed 2400\n",
      "processed 2450\n",
      "processed 2500\n",
      "processed 2550\n",
      "processed 2600\n",
      "processed 2650\n",
      "processed 2700\n",
      "processed 2750\n",
      "processed 2800\n",
      "processed 2850\n",
      "processed 2900\n",
      "processed 2950\n",
      "processed 3000\n",
      "processed 3050\n",
      "processed 3100\n",
      "processed 3150\n",
      "processed 3200\n",
      "processed 3250\n",
      "processed 3300\n",
      "processed 3350\n",
      "processed 3400\n"
     ]
    }
   ],
   "source": [
    "test_X = fs.transform(np.array([np.array(s) for s in testsignals]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"bioFeatures_test.csv\", \"w\") as f:\n",
    "    for idx,vals in enumerate(test_X):\n",
    "        s = str(idx)\n",
    "        for v in vals:\n",
    "            s += \",%f\" % v\n",
    "        s += \"\\n\"\n",
    "        f.write(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "addtest_X = np.loadtxt('data/rpeakfeature_test.csv', delimiter=',', skiprows=1, usecols=range(1,9))\n",
    "test_X_new = []\n",
    "for i in range(len(test_X)):\n",
    "     test_X_new.append(np.concatenate([test_X[i], addtest_X[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 0\n",
      "processed 50\n",
      "processed 100\n",
      "processed 150\n",
      "processed 200\n",
      "processed 250\n",
      "processed 300\n",
      "processed 350\n",
      "processed 400\n",
      "processed 450\n",
      "processed 500\n",
      "processed 550\n",
      "processed 600\n",
      "processed 650\n",
      "processed 700\n",
      "processed 750\n",
      "processed 800\n",
      "processed 850\n",
      "processed 900\n",
      "processed 950\n",
      "processed 1000\n",
      "processed 1050\n",
      "processed 1100\n",
      "processed 1150\n",
      "processed 1200\n",
      "processed 1250\n",
      "processed 1300\n",
      "processed 1350\n",
      "processed 1400\n",
      "processed 1450\n",
      "processed 1500\n",
      "processed 1550\n",
      "processed 1600\n",
      "processed 1650\n",
      "processed 1700\n",
      "processed 1750\n",
      "processed 1800\n",
      "processed 1850\n",
      "processed 1900\n",
      "processed 1950\n",
      "processed 2000\n",
      "processed 2050\n",
      "processed 2100\n",
      "processed 2150\n",
      "processed 2200\n",
      "processed 2250\n",
      "processed 2300\n",
      "processed 2350\n",
      "processed 2400\n",
      "processed 2450\n",
      "processed 2500\n",
      "processed 2550\n",
      "processed 2600\n",
      "processed 2650\n",
      "processed 2700\n",
      "processed 2750\n",
      "processed 2800\n",
      "processed 2850\n",
      "processed 2900\n",
      "processed 2950\n",
      "processed 3000\n",
      "processed 3050\n",
      "processed 3100\n",
      "processed 3150\n",
      "processed 3200\n",
      "processed 3250\n",
      "processed 3300\n",
      "processed 3350\n",
      "processed 3400\n"
     ]
    }
   ],
   "source": [
    "test_X2 = hrvfs.transform(np.array([np.array(s) for s in testsignals]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"hrvFeatures_test.csv\", \"w\") as f:\n",
    "    for idx,vals in enumerate(test_X2):\n",
    "        s = str(idx)\n",
    "        for v in vals:\n",
    "            s += \",%f\" % v\n",
    "        s += \"\\n\"\n",
    "        f.write(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X_new2 = []\n",
    "for i in range(len(test_X)):\n",
    "     test_X_new2.append(np.concatenate([test_X_new[i], test_X2[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_test = xgb.DMatrix(test_X_new2)\n",
    "y_pred = bst.predict(xg_test)\n",
    "f = open(\"submission.csv\", \"w\")\n",
    "f.write(\"id,y\\n\")\n",
    "for i,x in enumerate(y_pred):\n",
    "    f.write(\"{},{}\\n\".format(i,y_pred[i]))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
