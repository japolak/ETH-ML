\section*{\normalsize{Probability Modeling}}
\subsection*{Regression}
Ass: $(x_i,y_i) \text{ iid} \sim P(X,Y)$, Hypothesis:  $h:X\rightarrow Y$ \\
Min Prediction Error: $R(h) = \mathbb{E}_{x,y}[l(y;h(x))]$  \\
Cond. mean: $h^*(x) = \mathbb{E}[Y|X=x]$ (min for sq. loss)\\
Estimate: $\hat{P}(Y|X) $, Pred: $\hat{y} = \hat{\mathbb{E}}[Y|X=x]$

\subsection*{Maximum Likelihood Estimation (MLE)}
Conditional Likelihood $\hat{P}(Y|X,\theta)$, opt. param. w MLE

$\theta^* = \arg\max_\theta\:\hat{P}(Y|X,\theta) = 
\arg\min_\theta -\sum_{i}^n \log \hat{P}(y_i|x_i,\theta)$

\subsection*{MLE Gaussian}
Ass: noise $P(Y=y|X=x,\theta)=\mathcal{N}(y;h(x),\sigma^2)$\\
MLE: $ \hat{h} = \arg\min_{h \in \mathcal{H} } \sum_{i=1}^n(y_i - h(x_i))^2$\\
Linear: $h(x)=w^Tx$, $ Y=w^T X + \epsilon, \epsilon \sim \mathcal{N}(0, \sigma^2)$\\
MLE: $w^*=\arg\min_w \sum_{i=1}^n(y_i-w^Tx_i)^2$
\subsection*{$\color{black}{\text{Pred.Error}=\text{Bias}^2+\text{Variance}+\text{Noise}}$}
$ \mathds{E}[(Y-\hat{h})^2]=
\mathds{E}[\mathds{E}[\hat{h}]-h^*]^2+
\mathds{E}[(\mathds{E}[\hat{h}]-\hat{h})^2]+
\mathds{E}[Y-h^*]^2
$ 

\subsection*{Maximum a posteriori estimate (MAP)}
Prior: $w \sim P(w)$ s.t. $w \bot x$, eg. $w \sim \mathcal{N}(0,\beta^2)$\\
Posterior: $P(w|x,y)=\frac{P(w)P(y|x,w)}{P(y|x)} $ (Bayes Rule)\\
MAP: $w^*=\arg\max_w P(w|x,y) =$\\
$=\arg\min_w -\log P(w)-\log P(y|x,w)+const$

\subsection*{MAP Gaussian}
Ass: noise $P(y,x,w) \text{ iid.} \sim \mathcal{N}$, prior $P(w) \sim \mathcal{N}$\\
MAP: $ w^*=\arg\max_w P(w) \prod_i P(y_i|x_i,w) =$\\
$=\arg \min_w \frac{1}{2\sigma^2} \sum_{i=1}^n (y_i-w^Tx_i)^2 + \frac{1}{2\beta^2} \|w\|_2^2$

\subsection*{Regularization}
Optim: $\arg\min_w\sum_i^n l(w^Tx_i;x_i,y_i)+C(w)$\\
Prior: $C(w)=-\log P(w)$\\
Likelihood: $l(w^Tx_i;x_i,y_i)=-\log P(y_i|x_i,w) $

\subsection*{Classification}
Min Prediction Error: $ R(h)=\mathds{E}_{x,y}[[Y\neq h(x)]]$\\
$h^*(x)=\arg\min_{\hat{y}}\mathds{E}_y[[Y\neq \hat{y}]|X=x]=$\\
$=\arg\max_{\hat{y}}P(Y=\hat{y}|X=x)$

\subsection*{Logistic Regression}
Link function: $\sigma(w^Tx) = (1+exp(-w^Tx))^{-1}$\\
Ass: Bernoulli noise $P(y|x,w)=\text{Ber}(y;\sigma (w^Tx))$\\
Cond. dist: $P(y|x,\hat{w})=(1+\exp(-y\hat{w}^Tx))^{-1}$\\
Pred: $\hat{y}=\arg\max_{\hat{y}} P(\hat{y}|x,\hat{w})=\text{sign}(w^Tx)$\\
MLE: $w^* = \arg\min_w \sum_{i=1}^n l_{\log}(w)$

\subsection*{Logistic loss}
$l_{\log}$ is convex, everywhere diff., reg $z=-yw^Tx$ \\
$l_{\log}(z)=\log(1+e^{-z})\approx z \text{ for } z \gg 0 \text{, } \approx 0 \text{ for } z \ll 0 $\\
$l_{\log}(w)= \log(1+\exp(-y_iw^Tx_i))=P(Y=y|w,x)$\\
$\nabla_w l_{\log}=\frac{1}{1+\exp(yw^Tx)}(-yx) = \hat{P}(Y=-y|w,x)(-yx)
$

\subsection*{SGD Logistic Regression}
Initialize $w$, for $t=1,2...$: $(x,y)\in_{\text{unif.a.r}} D$\\
Missclassif prob: $\hat{P}(Y=-y|w,x)=(1+exp(yw^Tx))^{-1}$\\
Update:$w \leftarrow w + \eta_t y x\hat{P}(Y=-y|w,x) $

\subsection*{Cross-Entropy loss}
$l_{CE}(y;x,w_{1:c})=-\log P(Y=y|x,w_{1:c})$\\
Softmax: $P(Y=y|x,w_{1:c})=\frac{\exp(w_i^Tx)}{\sum_{j=1}^c \exp(w_j^Tx)}$