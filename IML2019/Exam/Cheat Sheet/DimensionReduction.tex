\section*{\normalsize{Dimension Reduction}}
\subsection*{Principal component analysis (PCA)}
Ass: $\mu =\frac{1}{n}\sum_{i = 1}^n x_i = 0$,   $\Sigma = \frac{1}{n}\sum_{i=1}^n x_i x_i^T = \frac{1}{n}X^TX$\\
Optim: $(w,z)=\arg \min_{\|w \|_2 =1, z} \sum_{i=1}^n || w z_i  - x_i||_2^2$\\
Sol: $ w = \arg \max_{\|w\|_2=1} w^T\Sigma w$, $w^* = v_1$, $z^*_i = w^Tx_i $\\
Optim: $(W,z_{1:n}) = \arg \min \sum_{i=1}^n ||W z_i - x_i||_2^2$,\\
Sol: $W = (v_1|...|v_k) \in \mathds{R}^{d\times k}$ is orthogonal \\
$\Sigma = \sum_{i=1}^d \lambda_i v_i v_i^T$ and $z_i = W^T x_i$\\
Linear mapping $f(x)=W^Tx$, SVD: $X=USV^T$

\subsection*{Kernel PCA}
Ass: $w=\sum_{j=1}^n \alpha_j \phi(x_j)$, $\|w\|_2^2 = \alpha^T K \alpha $\\
Optim: $ \arg\max_{\alpha^T K \alpha =1} \alpha^T K^T K \alpha$\\
Sol: $\alpha^{(i)}=\frac{1}{\sqrt{\lambda_i}}v_i$, $K=\sum_{i=1}^n \lambda_iv_iv_i^T $\\
New point $x$ projection as $z$: $z_i = \sum_{j=1}^n\alpha_j^{(i)}k(x,x_j)$

\subsection*{Autoencoders}
Learn identity function: $x \approx f(x;\theta)$\\
$f(x;\theta) = f_2(f_1(x;\theta_1);\theta_2)$; $f_1:$ en-, $f_2:$ de-coder\\
Optim: $ \min_W \sum_{i=1}^n||x_i-f(x_i;W)||_2^2$\\
Internal representation: $v=\varphi W^{(1)} x $